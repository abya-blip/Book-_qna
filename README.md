# Book-_qna

Projectâ€¯Recap & Rationale â€“ Building, Hardening, and (Soon)Â Deploying a Localâ€‘First Bookâ€¯Q&Aâ€¯RAG System

1â€¯.Â GenesisÂ & Goal
The original idea was simple: â€œAsk naturalâ€‘language questions about the full text of publicâ€‘domain books Iâ€™ve downloaded.â€ We quickly converged on a Retrievalâ€‘Augmented Generation (RAG) pipeline becauseâ€¯raw largeâ€‘languageâ€‘models hallucinate and canâ€™t hold gigabytes of prose in context. The RAG recipe is:

Ingest every book into a DB.

ChunkÂ +Â Embed the text; store vectors in a fast similarity index.

Retrieve the mostâ€‘relevant chunks for a user query.

Generate a final answer with an LLM that is grounded in those chunks.

All code had to run locally (no SaaS fees) yet remain deployâ€‘able. Hardware on hand: an i5â€‘12500HX, 16â€¯GBÂ RAM, an RTXÂ 3050Â (6â€¯GB), and Windows. This shaped many decisions.

2â€¯.Â Data LayerÂ â€“ Scraping, Cleaning & SQLite
Scraper: A lightweight Python script iterates over ProjectÂ Gutenberg pages, saving *.txt files to books/. We added a CLI arg to start at pageÂ 57 (your request) and a progress bar that calculates ETA from network speed (â‰ˆ1.2Â items/s on 150â€¯Mbps).

db.py:
Initialises a data/books.db file;
parses metadata (Title: / Author: tags or filename fallback);
inserts full text rows.
We verified >â€¯1â€¯400â€¯books imported (SQLite query COUNT(*) gave 1â€¯490).

Cleaning: Before vectorising we strip Gutenberg headers/footers and boilerâ€‘plate lines via regex. This prevents the LLM from parroting licence text.

3â€¯.Â VectorÂ StoreÂ â€“ FAISS + Sentenceâ€‘Transformers
Embedding model: sentence-transformers/all-MiniLM-L6-v2 â†’ 384â€‘dim vectors, good quality yet GPUâ€‘friendly. We cached it locally to avoid online download issues.

Chunking: RecursiveCharacterTextSplitter at 500â€¯chars with 100Â overlap balances semantic cohesion vs. index size.

Index build (rag.py): Batches of 64 vectors are added to a faiss.IndexFlatL2; metadata (bookÂ IDÂ + raw chunk) is pickled alongside. FAISS files are in data/faiss_index/.

Result: ~13â€¯k chunks; index size â‰ˆÂ 20â€¯MB; build time <â€¯2Â min on RTXÂ 3050 (CPU works too).

4â€¯.Â BackendÂ APIÂ â€“ FastAPI
We composed app/main.py in iterative passes:

CORS allowing localhost:5173.

Endpoints

/booksÂ â†’ list IDs, titles, authors.

/queryÂ (/ask)Â â†’ main QA endpoint.

/book/{id}Â â†’ preview raw text.

RAGÂ search function: embed query â†’ search FAISS â†’ clean + deduplicate topâ€‘k chunks.

LLMÂ call: local Ollama server (http://localhost:11434) running Mistralâ€‘7B. We posted {"model": "mistral", "prompt": prompt, "stream": false} and parsedÂ response.

Guardrails

Initial: strict prompt only answer from context.

Iteration: added distance threshold (DIST_THRES) and fallback logic.

Current:
StrictÂ mode â€“ if no relevant chunk from requested book, respond â€œanswer not availableâ€;
KnowledgeÂ fallback â€“ on your request we added an alt branch: the LLM may answer from its own knowledge but must preface with â€œ(General knowledgeÂ â€¦ )â€. No crossâ€‘book contamination.

All emoji logging was removed for Windows console compatibility.

5â€¯.Â FrontendÂ â€“ ReactÂ +Â ViteÂ +Â Tailwind
Bootstrapped with Vite/TypeScript.

Tailwind setup (postcss, config, darkâ€‘mode).

UIÂ evolution:
Basic form â†’ gradient BG, glass card, loader overlay.
Added live book search: a controlled input filters the <select> options in real time; queries update via axios.

States: books, search, selectedBook, question, answer, sources, loading.

Request flow: axios.post('/query', { book_id, question }) â†’ display answer or error.

6â€¯.Â Local LLMÂ â€“ Ollama
Installed with winget / brew.

Pulled mistral (ollama run mistral downloads ~4â€¯GB).

Verified GPU acceleration via print(torch.cuda.is_available()). For users without CUDA we fallback to CPU.

Ollama advantages: offline, free, easy REST API.

7â€¯.Â Deployment Strategy
We settled on RenderÂ +Â Vercel combo:

Backend on Render

Repo: app/, data/ (DB + FAISS) and requirements.txt.

startCommand: uvicorn app.main:app --host 0.0.0.0 --port 8000.

Considerations:
â€¢ Render canâ€™t run Ollama (needs GPU & root).
â€¢ For true cloud inference we plan Fireworks.ai; but for now weâ€™ll keep LLM local or on another VPS.

Frontend on Vercel

Separate repo frontend/.

VITE_API_URL env var â†’ Render backend URL.

Automatic HTTPS and global CDN.

For demonstration without cloud LLM we can expose local backend via ngrok.

8â€¯.Â LLM Hallucination Control
Problems encountered: generic/incorrect answers (â€œcharmingÂ heroineâ€ in TreasureÂ Island).
Fixes implemented:

Reduced context to topÂ 3Â chunks.

Strict prompt.

Distance gating.

Fallback message.

Optionally boost quality by swapping MiniLM embeddings for text-embedding-3â€‘small via Fireworks.

9â€¯.Â Challenges & Remedies
Challenge	Solution
Windows console canâ€™t print emojis â†’ crashes	Removed emojis, set PYTHONIOENCODING=utfâ€‘8.
CUDA â€œTorch not compiledâ€	Installed +cu118 wheel manually or forced CPU.
CORS errors	Added correct origin in FastAPI and axios base URL.
npm ENOENT after deleting frontend/	Reâ€‘initialised with npmÂ initÂ vite@latest, reâ€‘installed deps.
Tailwind missing PostCSS plugin	Installed @tailwindcss/postcss and wrote postcss.config.js.
FAISS path issues	Logged absolute paths; ensured rag.py and main.py use identical BASE_DIR.

10â€¯.Â Current State
Local dev: npm run dev (concurrently runs FastAPI + React).

Strict Q&A works for 1â€¯490 books; fallback flagged answers when context absent.

UI polished with search, darkâ€‘mode toggle, loading overlay.

Ready to push backend to Render, front to Vercel, or run entire stack offline with ngrok exposure.

11â€¯.Â Future Enhancements
Chunkâ€‘level citations inline (footnote ids in answer).

User uploads: accept PDF/EPUB, run RAG ingest on the fly.

Async embeddings via GPU queue for massive corpora.

Fireworks.ai integration for cloud Mistral; store API key in Render envÂ vars.

Automated nightly scrape via a Render cron job (or GitHub Action).

VueÂ /Â Svelte frontends for experimentation; or a Streamlit demo for rapid labs.

Analytics (who asks what) with SQLite events or PostHog.

Why This Architecture?
Separation of concerns: Python handles heavy compute & retrieval; React handles UX.

Localâ€‘first: No vendor lockâ€‘in; works offline; GPU utilised.

Switchable LLM: Ollama locally, Fireworks/Together in cloud â€” same prompt interface.

FAISS: proven ANN library, GPUâ€‘ready, small footprint.

Sentenceâ€‘Transformers: open, lightweight, no API cost.

SQLite: zeroâ€‘config; perfect for readâ€‘heavy catalogue.

Render + Vercel: free tiers, automatic HTTPS, GitOps workflow.

Conclusion
You now possess a robust, offlineâ€‘capable RAG system that ingests entire libraries, retrieves precise passages, and uses an LLM only as a summariser, not a rumor mill. By toggling a single flag you control strictness vs. flexibility, making it suitable for both scholarly applications and casual exploration. Deployment is one git push away; scaling to the cloud merely swaps the LLM endpoint. From a pile of plainâ€‘text classics to a modern, Reactâ€‘fronted conversational bookshelf â€• you built it all, debugged the edgeâ€‘cases, and learned every layer along the way. ğŸš€
